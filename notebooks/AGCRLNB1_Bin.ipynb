{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tamil-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AGCRLEnvBin import AGCRLEnv\n",
    "from DQNAgent import DQNAgent\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prerequisite-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('observations.pickle', 'rb') as handle:\n",
    "    obs = pickle.load(handle)\n",
    "with open('actions.pickle', 'rb') as handle:\n",
    "    actions = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clinical-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "assim_rl_actionspace=np.linspace(0,100,21)\n",
    "env=AGCRLEnv(obs,actions,\"assim_sp\",assim_rl_actionspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "severe-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.resetinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "EPISODES = 1\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 500  # steps\n",
    "SHOW_PREVIEW = False\n",
    "SAVE_MODEL_EVERY=5000\n",
    "MODEL_NAME=\"AGCRL_ASSIM_BIN\"\n",
    "MIN_REWARD = -5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cordless-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env,env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "WARNING:tensorflow:From /home/kaypee/miniconda3/envs/ds/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/AGCRL_ASSIM_BIN___100.00max____4.60avg____0.00min__1618418799.model/assets\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "INFO:tensorflow:Assets written to: models/AGCRL_ASSIM_BIN___100.00max____4.60avg____0.00min__1618419060.model/assets\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "INFO:tensorflow:Assets written to: models/AGCRL_ASSIM_BIN___100.00max____5.40avg____0.00min__1618419324.model/assets\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n"
     ]
    }
   ],
   "source": [
    "ep_rewards=[]\n",
    "acc_regret=[0]\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "    step=0\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        step+=1\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, len(env.action_space))\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        ep_rewards.append(reward)\n",
    "#         print(step%AGGREGATE_STATS_EVERY)\n",
    "        if (step % AGGREGATE_STATS_EVERY)==0:\n",
    "            print(step)\n",
    "            \n",
    "            average_reward = sum(ep_rewards)/len(ep_rewards)\n",
    "            min_reward = min(ep_rewards)\n",
    "            max_reward = max(ep_rewards)\n",
    "            ar=max_reward-average_reward\n",
    "            arval=acc_regret[-1]+ar\n",
    "            acc_regret.append(arval)\n",
    "            agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon,acc_regret=arval)\n",
    "            ep_rewards=[]\n",
    "            # Save model, but only when min reward is greater or equal a set value\n",
    "            if min_reward >= MIN_REWARD and step%SAVE_MODEL_EVERY==0:\n",
    "                agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-ranch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
